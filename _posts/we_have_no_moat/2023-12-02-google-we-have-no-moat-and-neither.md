---
title: Google "We Have No Moat, And Neither Does OpenAI" 
date: 2023-12-02 12:30:47 +09:00
modified: 2023-12-02 12:30:47 +09:00
tags: [NLP]
usemathjax: true

---

> 아래 내용들은 Google 내부에서 유출된 문서로 그 진위 여부를 확인한 내용이며, 회사 전체가 아닌 Google 직원의 의견일 뿐이라고 말하고 있다.

<br>

## We Have No Moat And neither does OpenAI

**"우리에게는 해자가 없으며, 이는 OpenAI도 마찬가지이다."**<br><br>

<center><img src="/assets/img/we_have_no_moat/1.jpeg"/></center>
<br>

여기서 말하는 **해자**란 어떤 기업이 가지고 있는 고유의 특허, 기술 등을 지칭하며 다른 기업이 쉽게 넘볼 수 없는, 진입장벽이 높은 유무형의 자산을 일컫는다.<br><br>

Google과 OpenAI가 arms race, 즉 군비 경쟁을 벌이는 동안 제 3의 세력인 **<u>오픈 소스 진영</u>**이 조용히 **<u>Major Open Problems</u>**라고 여겨지는 문제들을 해결하여 사람들에게 제공하고 있었다. 그 중 몇 가지 예는 다음과 같다. 

- **LLMs on a Phone** : 사람들은 초당 5토큰으로 Pixel 6에서 <u>Foundation Model</u>을 실행하고 있다. 
- **Scalable Personal AI** : 저녁에 노트북으로 개인화된 AI를 fine-tuning할 수 있다. 
- **Responsible Release** : 웹 사이트 전체가 아무런 제한 없이 Art model로 가득 차 있으며 텍스트도 크게 뒤처지지 않는다. 
- **Multimodality** : 현재 멀티모달 SienceQA SOTA는 한 시간 만에 학습되었다.

<br>
여전히 품질은 Google이 앞서고 있지만 격차는 점점 빠르게 좁혀지고 있다. <br><br>

제 3의 세력들은 우리가 1,000만 달러와 540B의 파라미터로 어려움을 겪던 일을 1억 달러와 13B의 매개변수로 몇 달이 아니라 몇 주 만에 해결하고 있다. 이는 다음과 같은 시사점을 가진다. 

- **Google에는 비법이 없다.** Google의 최선의 희망은 다른 사람들이 Google 외부에서 하고 있는 일에서 배우고 협력하는 것이다. 3P 통합을 활성화하는 데 우선순위를 두어야 한다.
- **사용 제한이 없는 고품질의 무료 대안이 있을 때 사람들은 제한된 모델에 비용을 지불하지 않을 것이다.**
- **Large model은 우리의 속도를 늦추고 있다.** 장기적으로 볼 때 가장 좋은 모델은 빠르게 반복할 수 있는 모델이다. 이제 20B 미만의 매개변수 체제에서 무엇이 가능한지 알았으니 작은 모델을 더 이상 사후 고려사항으로 두지 말아야 한다.

<br>

<center><img src="/assets/img/we_have_no_moat/chart.png"/></center>

<br>

## What Happend

2023년 3월, Meta의 **LLaMA**가 [대중에게 유출](https://www.vice.com/en/article/xgwqgw/facebooks-powerful-large-language-model-leaks-online-4chan-llama)되면서 오픈 소스 커뮤니티는 처음으로 제대로 된 기능을 갖춘 <u>foundation model</u>을 손에 넣었다. 이 모델에는 instruction이나 conversation tuning도 없었고, RLHF도 없었다. 그럼에도 불구하고 커뮤니티는 자신들이 받은 foundation model의 중요성을 즉시 이해했다.

그 후 엄청난 혁신이 쏟아져 나왔고, 주요 개발이 단 며칠 사이에 이루어졌다. 한 달도 채 지나지 않아, [Instruction tuning](https://crfm.stanford.edu/2023/03/13/alpaca.html), [Quantization](https://github.com/ggerganov/llama.cpp), [Quality improvements](https://lmsys.org/blog/2023-03-30-vicuna/), [Human evals](https://arxiv.org/pdf/2303.16199.pdf), [Multimodality](https://arxiv.org/pdf/2303.16199.pdf), [RLHF](https://drive.google.com/file/d/10iR5hKwFqAKhL3umx8muOWSRm7hs5FqX/view) 등 다양한 변형이 등장했으며, 이 중 상당수는 서로를 기반으로 발전하였다.

가장 중요한 것은 누구나 손댈 수 있을 정도로 스케일링 문제를 해결했다는 점이다. 새로운 아이디어의 대부분은 평범한 사람들에게서 나왔으며, 학습과 실험에 대한 진입 장벽이 대형 연구 기관의 총체적인 결과물에서 <u>한 사람, 저녁 시간, 구형 노트북</u>으로 낮아졌다.

<br>

## Why We Could Have Seen It Coming

이것은 여러 면에서 놀라운 일이 아니다. 현재 오픈 소스 LLM의 르네상스는 이미지 생성의 르네상스에 이어 뜨겁게 달아오르고 있으며, 커뮤니티에서도 많은 사람들이 이 시기를 LLM의 "**[Stable Diffusion Moment](https://simonwillison.net/2023/Mar/11/llama/)**"이라고 부른다.

두 경우 모두 **[Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)(LoRA)**이라는 훨씬 저렴한 Fine-Tuning 메커니즘과 스케일 면에서 획기적인 발전(이미지 합성의 경우 [latent diffusion](https://arxiv.org/abs/2112.10752), LLM의 경우 [Chinchilla](https://arxiv.org/abs/2203.15556))이 결합되어 저비용으로 대중의 참여가 가능해졌다. 두 경우 모두, 충분히 높은 품질의 모델에 대한 액세스는 전 세계 개인과 기관의 아이디어와 반복을 촉발시켰으며, 모두 대형 업체들을 빠르게 앞질렀다.

이러한 기여는 이미지 생성 분야에서 중추적인 역할을 했으며, **Stable Diffusion**은 Dall-E와는 다른 길을 걷게 되었다. 개방형 모델을 채택함으로써 제품 통합, 마켓플레이스, 사용자 인터페이스, 그리고 Dall-E에는 없던 혁신이 이루어졌다.

그리고 그 효과는 분명했다. 문화적 영향력 측면에서 빠르게 우위를 점한 OpenAI의 Dall-E는 점점 더 무의미해졌다. LLM에서도 같은 일이 일어날지는 아직 미지수이지만 큰 구조적 요소는 동일하다.

<br>

## What We Missed

최근 오픈소스의 성공을 이끈 혁신은 여전히 우리가 고민하고 있는 문제를 직접적으로 해결해 준다. 이러한 작업에 더 많은 관심을 기울인다면 같은 일을 반복하지 않을 수 있다.

### LoRA는 우리가 더 주목해야 할 믿을 수 없을 정도로 강력한 기술이다.

**[LoRA](https://arxiv.org/abs/2106.09685)**는 모델 업데이트를 <u>low-rank factorizations</u>로 표현하여 업데이트 행렬의 크기를 최대 수천 배까지 줄이는 방식으로 작동한다. 따라서 적은 비용과 시간으로 모델을 Fine-Tuning할 수 있다. 소비자 하드웨어에서 몇 시간 만에 언어 모델을 개인화할 수 있다는 것은 특히 새롭고 다양한 지식을 거의 실시간으로 통합해야 하는 경우 큰 의미가 있다. 그러나 이 기술이 Google의 가장 야심찬 프로젝트 중 일부에 직접적인 영향을 미치고 있음에도 불구하고 Google 내부에서 제대로 활용되지 않고 있다.

<br>

## Retraining models from scratch is the hard path

**모델을 처음부터 재학습하는 것은 어려운 방법이다.**

**LoRA**가 효과적인 이유 중 하나는 다른 형태의 Fine-Tuning과 마찬가지로 누적 학습이 가능하다는 점이다. instruction tuning과 같은 개선 사항을 적용한 다음 다른 기여자가 대화, 추론 또는 도구 사용을 추가할 때 이를 활용할 수 있다. 개별적인 Fine-Tuning은 low-rank이지만, 그 총합은 그럴 필요가 없으므로 시간이 지남에 따라 모델에 대한 full-rank 업데이트가 누적될 수 있다.

즉, 새롭고 더 나은 데이터 셋과 태스크를 사용할 수 있게 되면 <u>전체 실행 비용을 지불하지 않고도 모델을 저렴하게 최신 상태로 유지</u>할 수 있다.

반면, 거<u>대한 모델을 처음부터 학습시키면 사전 학습뿐만 아니라 그 위에 이루어진 반복적인 개선 사항도 모두 버려지게 된다.</u> 오픈 소스 세계에서는 이러한 개선 사항이 지배적이기까지 오래 걸리지 않으므로 전체 재학습에 엄청난 비용이 소요된다.

우리는 각각의 새로운 애플리케이션이나 아이디어에 대해 정말로 완전히 새로운 모델이 필요한지 신중하게 생각해야 한다. 모델 가중치를 직접 재사용할 수 없을 정도로 아키텍처가 크게 개선되었다면 이전 세대의 기능을 최대한 유지할 수 있는 보다 공격적인 형태의 <u>distillation</u>에 투자해야 한다.

<br>

## Large models aren’t more capable in the long run if we can iterate faster on small models

**작은 모델을 더 빠르게 반복할 수 있다면 큰 모델은 장기적으로 더 나은 성능을 발휘할 수 없다.**

**LoRA 업데이트는** 가장 인기 있는 모델 사이즈의 경우 제작 비용이 매우 저렴하다(~$100). 즉, 아이디어만 있으면 거의 모든 사람이 LoRA를 이용하여 모델을 제작하고 배포할 수 있으며, 학습 시간은 하루가 채 걸리지 않는 것이 일반적이다. 이 정도 속도라면 이러한 모든 Fine-Tuning의 누적 효과가 모델의 크기로 인한 단점을 극복하는 데 그리 오랜 시간이 걸리지 않는다. 실제로 엔지니어 시간 측면에서 볼 때, 이러한 모델의 개선 속도는 가장 큰 모델로 할 수 있는 것보다 훨씬 빠르며, [최고의 모델은 이미 ChatGPT와 거의 구별할 수 없을 정도이다.](https://bair.berkeley.edu/blog/2023/04/03/koala/) 지구상에서 가장 큰 모델을 유지 관리하는 데 집중하면 오히려 불리한 상황에 처하게 된다.

<br>

## Data quality scales better than data size

**데이터 품질은 데이터 크기보다 더 잘 확장된다.**

프로젝트 중 상당수는 고도로 선별된 소규모 데이터 셋을 학습하여 시간을 절약하고 있다. 이는 데이터 확장 법칙에 어느 정도 유연성이 있음을 시사한다. 이러한 데이터 셋의 존재는 **'데이터는 생각대로 동작하지 않는다'**의 사고 방식에서 비롯된 것으로, Google 외부에서 학습을 수행하는 표준 방식으로 빠르게 자리 잡고 있다. 이러한 데이터 셋은 합성 방법(예: 기존 모델에서 최상의 응답을 필터링)과 다른 프로젝트에서 스크래빙을 통해 구축되며, 이 두 가지 방법 중 어느 것도 Google에서 널리 사용되는 방법은 아니지만 다행히도 이러한 고품질 데이터 셋은 오픈 소스이므로 무료로 사용할 수 있다.

<br>

## Directly Competing With Open Source Is a Losing Proposition

**오픈소스와 직접 경쟁하는 것은 손해이다.**

이러한 최근의 진전은 Google의 비즈니스 전략에 직접적이고 즉각적인 영향을 미친다. <u>사용 제한이 없는 고품질의 무료 대안이 있는데 누가 사용 제한이 있는 Google 제품에 비용을 지불할 것인가?</u>

그리고 우리가 따라잡을 수 있을 거라고 기대해서는 안된다. 현대 인터넷이 오픈소스로 운영되는 데에는 이유가 있으며, 오픈소스에는 우리가 복제할 수 없는 몇 가지 중요한 이점이 있다.

<br>

## We need them more than they need us

**고객이 우리를 필요로 하는 것보다 우리가 고객을 더 필요로 한다.**

기술을 비밀로 유지하는 것은 항상 어려운 일이었다. Google 연구원들은 정기적으로 다른 회사로 이직하기 때문에 우리가 알고 있는 모든 것을 알고 있고, 파이프라인이 열려 있는 한 계속 그럴 것이라고 생각할 수 있다.

하지만 LLM의 최첨단 연구 비용이 저렴해지면서 기술 경쟁 우위를 유지하는 것은 더욱 어려워졌다. 전 세계의 연구 기관들이 서로의 연구를 기반으로 솔루션 영역을 넓혀가고 있으며, 우리의 역량을 훨씬 능가하는 폭넓은 방식으로 솔루션을 탐색하고 있다. 우리는 외부의 혁신이 그 가치를 희석시키는 동안 우리의 비밀을 굳건히 지키려고 노력할 수도 있고, 서로에게서 배우려고 노력할 수도 있다.

<br>

## Individuals are not constrained by licenses to the same degree as corporations

**개인은 기업과 같은 수준의 라이센스 제약을 받지 않는다.**

이러한 혁신의 대부분은 Meta에서 유출된 모델의 가중치를 기반으로 이루어지고 있다. [진정한 개방형 모델](https://bigscience.huggingface.co/blog/bloom)이 개선됨에 따라 이러한 상황은 필연적으로 변화하겠지만, 중요한 것은 기다릴 필요가 없다는 것이다. **'개인적 사용'**이 제공하는 법적 보호와 개인에 대한 기소의 비현실성 때문에 개인은 이러한 기술이 뜨거울 때 접근하고 있다.

<br>

## Being your own customer means you understand the use case

**고객이 된다는 것은 유스 케이스를 이해한다는 의미이다.** 

이미지 생성 분야에서 사람들이 만드는 모델을 살펴보면 애니메이션 제너레이터부터 HDR 랜드스케이프에 이르기까지 방대한 창의성이 쏟아져 나오고 있다. 이러한 모델은 특정 하위 장르에 깊이 몰입한 사람들이 사용하고 만들었기 때문에 우리가 따라올 수 없는 깊이 있는 지식과 공감을 제공한다.

<br>

## Owning the Ecosystem: Letting Open Source Work for Us

역설적이게도 이 모든 것에서 확실한 승자는 **Meta**이다. 유출된 모델이 자신들의 것이었기 때문에, 그들은 사실상 <u>지구 전체에 해당하는 무료 노동력을 확보한 셈</u>이다. 대부분의 오픈소스 혁신이 Meta의 아키텍처를 기반으로 이루어지고 있기 때문에, Meta가 이를 자사 제품에 통합하는 것을 막을 수 있는 방법은 없다.

생태계를 소유하는 것의 가치는 아무리 강조해도 지나치지 않는다. Google은 Chrome 및 Android와 같은 오픈 소스 제품에서 이 패러다임을 성공적으로 활용하고 있다. 혁신이 일어나는 플랫폼을 소유함으로써 Google은 thought leader이자 direction-setter로서의 입지를 굳히고, 자신보다 더 큰 아이디어에 대한 내러티브를 형성할 수 있는 능력을 얻게 된다.

**모델을 더 엄격하게 제어할수록 개방형 대안 모델을 더 매력적으로 만들 수 있다.** Google과 OpenAI는 모두 모델 사용 방식을 엄격하게 통제할 수 있는 릴리스 패턴에 방어적으로 끌려왔다. 하지만 이러한 통제는 허구이다. <u>승인되지 않은 목적으로 LLM을 사용하고자 하는 사람은 누구나 자유롭게 사용할 수 있는 모델 중 원하는 것을 선택하면 된다.</u>

Google은 오픈 소스 커뮤니티의 리더로서 폭넓은 대화를 무시하지 말고 협력하여 주도권을 잡아야 한다. 이는 아마도 작은 ULM 변형에 대한 모델 가중치를 게시하는 것과 같은 불편한 조치를 취하는 것을 의미할 것이다. 이는 모델에 대한 일부 통제권을 포기하는 것을 의미한다. 하지만 이러한 타협은 불가피하다. 혁신을 주도하면서 동시에 혁신을 통제할 수는 없기 때문이다.

<br>

## Epilogue: What about OpenAI?

오픈소스에 대한 이 모든 이야기는 OpenAI의 현재 폐쇄적인 정책을 고려할 때 불공평하게 느껴질 수 있다. <u>저들은 공유하지 않을 텐데 우리는 왜 공유해야 할까?</u> 하지만 사실 우리는 이미 고급 연구원이 꾸준히 유출되는 형태로 모든 것을 그들과 공유하고 있다. 이러한 흐름을 막기 전까지는 비밀 유지에 대한 논의는 무의미하다.

결국 OpenAI는 중요하지 않다. 오픈소스와 관련하여 그들도 우리와 같은 실수를 저지르고 있으며, 우위를 유지할 수 있는 능력에 의문이 제기될 수밖에 없다. 오픈소스 대안이 그들의 입장을 바꾸지 않는 한 결국에는 오픈소스가 그들을 잠식할 수 있고 잠식할 것이다. 이 점에서 적어도 우리가 먼저 움직일 수 있다.

<br>

### Reference

- <https://www.semianalysis.com/p/google-we-have-no-moat-and-neither>

<br>