---
title: "[리뷰] OpenVLA: An Open-Source Vision-Language-Action Model"
date: 2025-06-18 12:00:00 +09:00
modified: 2024-06-18 12:00:00 +09:00
tags: 
    - ROBOT
    - VLA
usemathjax: true
---

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


>Moo Jin Kim, et al. "OpenVLA: An Open-Source Vision-Language-Action Model" arXiv preprint arXiv:2406.09246, 2024. [[paper]](https://arxiv.org/abs/2406.09246)

[1. Introduction](#1-introduction)  
[2. OpenVLA Model](#2-openvla-model)
  - [2.1 Preliminaries: Vision-Language Models](#21-preliminaries-vision-language-models)
  - [2.2 OpenVLA Training Procedure](#22-openvla-training-procedure)
  - [2.3 Training Data](#23-training-data)
  - [2.4 OpenVLA Design Decisions](#24-openvla-design-decisions)
  - [2.5 Infrastructure for Training and Inference](#25-infrastructure-for-training-and-inference)  

[3. The OpenVLA Codebase](#3-the-openvla-codebase)  
[4. Experiments](#4-experiments)
  - [4.1 Direct Evaluations on Multiple Robot Platforms](#41-direct-evaluations-on-multiple-robot-platforms)
  - [4.2 Data-Efficient Adaptation to New Robot Setups](#42-data-efficient-adaptation-to-new-robot-setups)
  - [4.3 Parameter-Efficient Fine-Tuning](#43-parameter-efficient-fine-tuning)
  - [4.4 Memory-Efficient Inference via Quantization](#44-memory-efficient-inference-via-quantization)

[5. Discussion and Limitations](#5-discussion-and-limitations)  
[6. Conclusions](#6-conclusions)

<br>

# 1. Introduction
로봇 학습 정책의 주요 약점은 훈련 데이터를 넘어선 상황에 대한 일반화 능력 부족이다. 기존의 강력한 **Vision-Language-Action Model(VLA)**들은 이러한 일반화 가능성을 보여주었지만, <u>대부분 폐쇄형(closed-source)</u>이고 <u>새로운 환경과 작업에 적용하기 위한 가이드라인이 부족</u>하다는 한계가 있다.

본 논문은 이러한 문제를 해결하고자 **OpenVLA**를 제안한다. OpenVLA는 오픈소스이면서도 기존 최고 성능을 뛰어넘는 일반 로봇 조작(robot manipulation) 정책이다. 대규모의 다양한 로봇 조작 데이터셋으로 파인 튜닝되어 SOTA 성능을 달성했으며, 특히 이전 연구에서 다루지 않았던 효율적인 파인 튜닝 전략과 소비자 GPU에서도 활용 가능한 계산 효율적 방법론을 제시한다.

<br>

# 2. OpenVLA Model
이 섹션에서는 Open X-Embodiment 데이터셋의 97만 개 로봇 시연 데이터로 훈련된 7B 규모의 VLA 모델인 **OpenVLA**를 소개한다. VLA 개발에는 최적의 모델 백본, 데이터셋, 하이퍼파라미터 선택 등 아직 충분히 탐구되지 않은 질문들이 많으므로, OpenVLA 개발 과정과 주요 학습 내용을 상세히 설명하고자 한다. 구체적으로는 OpenVLA의 기반이 되는 최신 VLM을 간략히 살펴보고(2.1), 기본적인 훈련 방식과 데이터셋(2.2, 2.3), 주요 설계 결정(2.4), 훈련 및 추론 인프라(2.5) 순으로 논의한다.

<br>

### 2.1 Preliminaries: Vision-Language Models
OpenVLA는 `Prismatic-7B` VLM을 기반으로 한다. 최신 VLM은 일반적으로 세 가지 주요 구성 요소로 이루어진다:

1. **Visual Encoder**: 이미지 입력을 여러 '이미지 패치 임베딩'으로 변환
2. **Projector**: 시각적 인코더의 출력 임베딩을 언어 모델의 입력 공간으로 매핑
3. **LLM backbone**: 텍스트 정보 처리  
   VLM은 다양한 인터넷 소스에서 수집된 이미지-텍스트 쌍 또는 인터리브된 데이터에 대해 다음 텍스트 토큰을 예측하는 목표로 end-to-end 훈련된다.

`Prismatic-7B` 는 600M 파라미터의 Visual Encoder, 작은 2-layer MLP Projector, 그리고 7B 파라미터의 Llama 2 언어 모델 백본을 사용한다. 특히 Prismatic의 시각적 인코더는 사전 훈련된 **SigLIP**과 **DinoV2** 모델로 구성된 두 부분으로 이루어져 있으며, 입력 이미지 패치는 두 인코더를 개별적으로 통과한 후 결과 피처 벡터가 채널별로 결합된다. DinoV2 피처를 추가하는 것은 기존 CLIP이나 SigLIP 단독 인코더와 비교하여 향상된 공간 추론 능력을 제공하는 것으로 나타났으며, 이는 로봇 제어에 특히 유용할 수 있다.

<u>SigLIP, DinoV2, Llama 2</u>는 각각 인터넷에서 제공되는 <u>image-text, image-only, text-only</u> 데이터의 수조 개 토큰으로 구성될 가능성이 높은 방대한 데이터로 사전 훈련되었으나, 훈련 데이터의 세부 정보는 공개되지 않았다. Prismatic VLM 자체는 이러한 구성 요소들 위에 LLaVA 1.5 데이터 혼합 (오픈소스 데이터셋에서 가져온 약 100만 개의 image-text 및 text-only 데이터 샘플 포함)을 사용하여 파인 튜닝되었다.

<br>

### 2.2 OpenVLA Training Procedure
OpenVLA를 훈련하기 위해, 로봇 행동 예측을 위해 사전 훈련된 `Prismatic-7B` VLM 백본을 파인 튜닝한다. 주요 훈련 절차는 다음과 같다.

- **행동 예측 공식화**:
  - 입력 관찰 이미지와 자연어 작업 명령을 예측된 로봇 행동 문자열에 매핑하는 **"vision-language"** 작업으로 정의함
- **로봇 행동 표현**:
  - **연속적 행동의 이산화**: VLM의 언어 모델 백본이 로봇 행동을 예측하도록, 연속적인 로봇 행동을 이산 토큰으로 매핑함
    - 각 행동 차원을 256개의 bin으로 개별 이산화
    - **Bin 너비 설정**: 훈련 데이터에서 행동의 1번째와 99번째 분위수 사이 간격을 균등하게 분할(Brohan 외 연구진이 사용한 최소-최대 경계 대신 분위수 사용은 이상치 행동 무시 효과)
  - **토큰 매핑**: N차원 로봇 행동은 N개의 이산 정수 \\(∈ [0 ... 255]\\) 로 변환됨
    - **Llama 토크나이저의 한계**: 파인 튜닝 시 새로운 토큰용 “special tokens”가 100개로 제한되어 256개 행동 토큰에 부족
    - **해결책**: Llama 토크나이저 어휘에서 가장 적게 사용되는 256개 토큰을 행동 토큰으로 덮어쓰는 Brohan 등의 방식을 따름
- **훈련 목표**:
  - 행동이 토큰 시퀀스로 처리된 후, OpenVLA는 예측된 행동 토큰에 대해서만 교차 엔트로피 손실을 평가하여 standard next-token prediction 목표로 훈련된다.

<br>

### 2.3 Training Data
OpenVLA 훈련 데이터셋 구성의 목표는 로봇 형태, 장면, 태스크의 큰 다양성을 포착하여, 최종 모델이 다양한 로봇을 즉시 제어하고 새로운 로봇 태스크에 효율적으로 파인 튜닝될 수 있도록 하는 것이다.

- **기반 데이터셋**:
  - **Open X-Embodiment dataset (OpenX)**를 활용
    - **작성 시점 기준**: 70개 이상 개별 로봇 데이터셋, 2백만 개 이상 로봇 궤적
- **데이터 큐레이션**: 원시 데이터셋에 여러 단계의 큐레이션을 적용하여 훈련을 실용적으로 만듦
  - **큐레이션 목표**:
    1. 모든 훈련 데이터셋에서 일관된 입력 및 출력 공간 보장
    2. 최종 훈련 혼합에서 형태, 장면, 태스크의 균형 잡힌 혼합 보장
  - **입출력 공간 일관성 (목표 1)**: OpenX, Octo를 따라 훈련 데이터셋을 다음 기준으로 제한함
    - 최소 하나의 3인칭 카메라를 가진 조작 데이터셋
    - 단일 암 end-effector 제어 사용
  - **균형 잡힌 혼합 (목표 2)**: 첫 필터링 단계를 통과한 모든 데이터셋에 Octo의 데이터 혼합 가중치를 활용
    - **Octo 방식**: 경험적으로 다양성이 낮은 데이터셋의 가중치를 낮추거나 제거하고, 태스크 및 장면 다양성이 큰 데이터셋의 가중치를 높임
- **추가 데이터셋 실험**:
  - Octo 출시 후 OpenX에 추가된 DROID 데이터셋 등을 훈련 데이터에 혼합 (보수적 혼합 가중치 10%)
  - **결과**: DROID에 대한 행동 토큰 정확도가 훈련 내내 낮게 유지됨. 이는 향후 다양성 확보를 위해 더 큰 혼합 가중치나 모델이 필요할 수 있음을 시사함
  - 따라서, 최종 모델 품질의 저하를 막기 위해 훈련 마지막 1/3 동안 데이터 혼합에서 DROID를 제거함

사용된 데이터셋 및 혼합 가중치에 대한 전체 개요는 다음 표와 같다. 
<figure align="center">
<center><img src="/assets/img/openvla/1.png" style="zoom: 50%;" /></center>
<figcaption>Table 3: OpenVLA training data mixture using datasets from the Open X-Embodiment dataset, following with a few additions</figcaption>
</figure>


<br>

### 2.4 OpenVLA Design Decisions
OpenVLA 모델 개발 시, 최종 모델 훈련 실행 전에 소규모 실험을 통해 다양한 설계 결정을 탐색했다. 반복 속도를 높이고 계산 비용을 줄이기 위해 초기 실험은 전체 OpenX 혼합 대신 BridgeData V2에서 OpenVLA 모델을 훈련하고 평가했다. 이러한 탐색에서 얻은 주요 학습 내용은 다음과 같다.

- **VLM Backbone:**
  - **실험 대상**: Prismatic, IDEFICS-1, LLaVA
  - **결과:**
    - LLaVA와 IDEFICS-1은 단일 객체 작업에서는 유사한 성능을 보임.
    - LLaVA는 다중 객체 및 언어 지침 기반 객체 조작 작업에서 IDEFICS-1보다 강력한 언어 접지(language grounding) 능력을 보임 (BridgeData V2 싱크 환경 5개 작업 평균 절대 성공률 35% 향상).
    - 파인 튜닝된 Prismatic VLM 정책은 단일 객체 및 다중 객체, 언어 접지 작업 모두에서 LLaVA 정책보다 약 10% 더 높은 절대 성공률을 보임.
  - **Prismatic 선택 이유:**
    - **성능 우위**: 융합된 SigLIP-DinoV2 백본이 제공하는 향상된 공간 추론 능력 덕분으로 추정 (2.1절 참조)
    - **사용 편의성**: 모듈식이고 사용하기 쉬운 코드베이스 제공
  - **결론**: OpenVLA 모델의 백본으로 Prismatic을 최종 선택함.
- **Image Resolution:**
  - **고려 사항**: 입력 이미지 해상도는 VLA 훈련의 계산 요구 사항에 큰 영향을 미침 (고해상도 → 더 많은 이미지 패치 토큰 → 더 긴 컨텍스트 길이 → 훈련 계산량 2차 증가)
  - **실험**: 224 × 224px 와 384 × 384px 입력 비교
  - **결과**: 평가에서 성능 차이 없었으나, 후자는 훈련 시간이 3배 더 소요됨.
  - **결론**: 최종 OpenVLA 모델의 해상도는 224 × 224px로 선택함. (많은 VLM 벤치마크와 달리, VLA에서는 해상도 증가가 성능 향상으로 이어지지 않음)
- **Fine-Tuning Vision Encoder:**
  - **기존 VLM 연구**: VLM 훈련 중 비전 인코더를 고정하는 것이 일반적으로 더 높은 성능을 보임 (인터넷 규모 사전 훈련에서 학습된 강력한 특징 보존).
  - **VLA 실험 결과**: VLA 훈련 중 비전 인코더를 파인 튜닝하는 것이 우수한 VLA 성능에 매우 중요함을 발견함.
  - **가설**: 사전 훈련된 비전 백본이 정확한 로봇 제어를 위한 장면의 중요한 부분에 대한 충분히 세분화된 공간 세부 정보를 캡처하지 못할 수 있음.
- **Training Epochs:**
  - **일반 LLM/VLM 훈련**: 훈련 데이터셋을 통해 최대 1~2 에포크 완료
  - **VLA 훈련 결과**: 훈련 데이터셋을 통해 훨씬 더 많이 반복하는 것이 중요하며, 실제 로봇 성능은 훈련 동작 토큰 정확도가 95%를 초과할 때까지 지속적으로 증가함을 발견함.
  - **최종 실행**: 훈련 데이터셋을 통해 27 에포크 완료
- **Learning Rate:**
  - **실험**: 여러 자릿수에 걸쳐 학습률 스윕
  - **결과**: 2e-5의 고정 학습률 (VLM 사전 훈련 시 사용된 학습률과 동일)에서 최상의 결과를 얻음.
  - **추가 발견**: learning rate warmup은 이점을 제공하지 않음.

<br>

### 2.5 Infrastructure for Training and Inference
- **훈련 인프라:**
  - 최종 OpenVLA 모델은 64개의 A100 GPU 클러스터에서 14일 동안 훈련됨
  - 총 소요 시간: 21,500 A100-hours
  - 배치 크기: 2048
- **추론 인프라:**
  - **메모리 요구 사항**: bfloat16 정밀도 (양자화 없음)로 로드 시 15GB의 GPU 메모리 필요.
  - **추론 속도**: 하나의 NVIDIA RTX 4090 GPU에서 약 6Hz로 실행 (컴파일, 추측 디코딩 등 최적화 없음).
  - **메모리 최적화**: 양자화를 통해 추론 중 OpenVLA의 메모리 공간을 더욱 줄일 수 있으며, 이는 실제 로봇 공학 작업에서 성능을 저하시키지 않음 (4.4절 참조).
  - **GPU별 추론 속도**: 그림 6에서 다양한 소비자 및 서버 등급 GPU의 추론 속도 보고
- **원격 추론 서버:**
  - 로봇에 대한 실시간 원격 스트리밍 동작 예측을 허용하는 원격 VLA 추론 서버 구현
  - **장점**: 로봇 제어를 위해 강력한 로컬 컴퓨팅 장치 불필요

<br>

# 3. The OpenVLA Codebase
모델과 함께, VLA 모델 훈련을 위한 모듈식 PyTorch 코드베이스인 **OpenVLA 코드베이스**를 공개한다(https://openvla.github.io 참조).

- **확장성**:
  - 개별 GPU에서의 VLA 파인 튜닝부터 다중 노드 GPU 클러스터에서의 수십억 파라미터 VLA 훈련까지 지원한다.
- **최신 기술 지원**:
  - AMP (Automatic Mixed Precision)
  - FlashAttention
  - FSDP (Fully Sharded Data Parallelism)
- **주요 기능**:
  - Open X 데이터셋에서의 훈련을 완벽하게 지원
  - HuggingFace의 AutoModel 클래스와 통합
  - LoRA 파인 튜닝 지원
  - 양자화된 모델 추론 지원

<br>

# 4. Experiments
실험 평가는 OpenVLA가 강력한 멀티 로봇 제어 정책으로서 즉시 사용 가능한지, 그리고 새로운 로봇 작업에 대한 파인 튜닝을 위한 좋은 초기화가 될 수 있는지 테스트하는 것을 목표로 한다. 구체적으로 다음 질문에 답하고자 한다.

1. OpenVLA는 여러 로봇과 다양한 유형의 일반화에 대해 평가할 때 기존의 제너럴리스트 로봇 정책과 비교하여 어떠한가?
2. OpenVLA는 새로운 로봇 설정 및 작업에서 효과적으로 파인 튜닝될 수 있으며, SOTA data-efficient 모방 학습 접근 방식과 비교하여 어떠한가?
3. 파라미터 효율적인 파인 튜닝 및 양자화를 사용하여 OpenVLA 모델의 훈련 및 추론에 대한 계산 요구 사항을 줄이고 더 쉽게 접근할 수 있도록 할 수 있는가? 성능-계산량 간의 트레이드오프는 무엇인가?

<br>

### 4.1 Direct Evaluations on Multiple Robot Platforms
<figure align="center">
<center><img src="/assets/img/openvla/2.png" style="zoom: 80%;" /></center>
<figcaption>Figure 3: BridgeData V2 WidowX robot evaluation tasks and results.</figcaption>
</figure>

<br>

- **Robot Setups and Tasks**:
  - **평가 플랫폼**:
    1. WidowX 로봇 (BridgeData V2 평가)
    2. Google 로봇 (모바일 조작 로봇, RT-1 및 RT-2 평가)
  - **평가 기준**: 다양한 일반화 축을 다루는 포괄적인 평가 작업 세트 정의.
    - 시각적 일반화 (보이지 않는 배경, 주의를 끄는 객체, 객체 색상/모양)
    - 동작 일반화 (보이지 않는 객체 위치/방향)
    - 물리적 일반화 (보이지 않는 객체 크기/모양)
    - 의미적 일반화 (보이지 않는 대상 객체, 지침 및 인터넷 개념)
    - 언어 조건화 능력 (여러 객체가 있는 장면에서 사용자의 프롬프트에 지정된 올바른 대상 객체 조작)
  - **평가 규모**:
    - **BridgeData V2**: 각 방법당 170개 롤아웃 (17개 작업, 각 10회 시행)
    - **Google 로봇**: 각 방법당 60개 롤아웃 (12개 작업, 각 5회 시행)
  - **평가 방식**: 모든 평가는 공정한 비교를 위해 동일한 작업과 동일한 초기 로봇 및 객체 상태 세트를 사용하여 A/B 평가로 수행됨.
- **비교 대상**:
  - **RT-1-X (35M 파라미터)**: OpenX 데이터셋 하위 집합에서 처음부터 훈련된 트랜스포머 정책
  - **RT-2-X (55B 파라미터)**: 인터넷에서 사전 훈련된 vision 및 language backbone을 활용하는 SOTA 클로즈드 소스 VLA
  - **Octo (93M 파라미터)**: OpenX 데이터셋 하위 집합에서 처음부터 훈련된 트랜스포머 정책. 오픈 소스 조작 정책 중 SOTA 모델.
- **결과** (그림 3: BridgeData V2, 그림 4: Google 로봇):
  <figure align="center">
  <center><img src="/assets/img/openvla/3.png" style="zoom: 60%;" /></center>
  <figcaption>Figure 4: Google robot evaluation results.</figcaption>
  </figure>
  - **RT-1-X 및 Octo**: 테스트된 작업에서 어려움을 겪음. 특히 방해 요소가 있을 때 올바른 객체를 조작하지 못하고, 로봇이 팔을 목적 없이 흔드는 경우 발생. (본 평가가 이전 연구보다 훨씬 큰 일반화 정도를 테스트하므로, 인터넷 사전 훈련 없는 모델의 낮은 성능은 예상됨)
  - **RT-2-X**: RT-1-X와 Octo를 명확히 능가하여, 로봇 공학을 위한 대규모 사전 훈련된 VLM의 이점을 보여줌.
  - **OpenVLA**:
    - Google 로봇 평가에서 RT-2-X와 유사한 성능을 보임.
    - BridgeData V2 평가에서는 RT-2-X보다 훨씬 뛰어난 성능을 보임 (7B vs 55B 파라미터).
    - 정성적으로, RT-2-X와 OpenVLA 모두 다른 테스트된 모델보다 훨씬 더 강력한 동작을 보임 (ex. 방해 객체 존재 시 올바른 객체 접근, 대상 객체 방향에 맞춘 end-effector 정렬, 실수로부터의 복구).
  - **RT-2-X의 우위**: 의미론적 일반화 작업에서 더 높은 성능 달성 (그림 3). 이는 더 큰 규모의 인터넷 사전 훈련 데이터 사용 및 로봇 작업 데이터와 인터넷 사전 훈련 데이터 모두와 함께 공동 파인 튜닝되어 사전 훈련 지식을 더 잘 보존하기 때문 (OpenVLA는 로봇 데이터에서만 파인 튜닝됨)
  - **OpenVLA의 우위**: BridgeData V2 및 Google 로봇 평가 모두에서 다른 모든 작업 범주에서 RT-2-X와 비슷하거나 더 나은 성능을 보임.
  - **성능 차이 원인 추정**:
    - 더 큰 훈련 데이터셋 (OpenVLA: 970k trajectories vs. RT-2-X: 350k trajectories).
    - 더 신중한 훈련 데이터셋 정리 (ex. Bridge 데이터셋에서 all-zero 액션 필터링)
    - 사전 훈련된 의미론적 및 공간적 특징을 결합한 융합된 비전 인코더 사용

<br>

### 4.2 Data-Efficient Adaptation to New Robot Setups
이전 연구는 주로 VLA를 "**out-of-the-box**"로 평가하는 데 중점을 두었지만, 새로운 작업 및 로봇 설정에 대한 VLA 모델의 효과적인 파인 튜닝은 광범위한 채택에 매우 중요하다. 이 섹션에서는 새로운 실제 로봇 설정에 빠르게 적응할 수 있는 OpenVLA의 능력을 조사한다.

<figure align="center">
<center><img src="/assets/img/openvla/4.png" style="zoom: 80%;" /></center>
<figcaption>Figure 5: Adapting to new robot setups.</figcaption>
</figure>

<br>

- **Robot setups and tasks**:
  - **파인 튜닝 레시피**: 목표 작업의 10–150개 시연 데이터가 포함된 작은 데이터셋을 사용하여 모든 모델 파라미터를 완전히 파인 튜닝함 (그림 5 참조; 파라미터 효율적 파인 튜닝은 4.3절에서 논의).
  - **테스트 설정**:
    1. **Franka-Tabletop**: 고정된 테이블 장착형 Franka Emika Panda 7-DoF 로봇 팔.
    2. **Franka-DROID**: 최근 공개된 DROID 데이터셋의 Franka 로봇 팔 설정으로, 이동 가능한 스탠딩 데스크에 장착됨.
  - **제어 주파수**: 각 설정은 각각 5Hz 및 15Hz의 비차단 컨트롤러 사용.
  - **Franka 선택 이유**: Franka 로봇 팔은 로봇 학습 커뮤니티에서 널리 사용되므로 OpenVLA 파인 튜닝의 "target"이 될 가능성이 높음. 다양한 제어 주파수 설정 테스트는 OpenVLA의 다양한 사용 사례 적용 가능성 평가 목적.
- **비교 대상**:
  - **Diffusion Policy**: 처음부터 훈련된 SOTA data-efficient 모방 학습 접근 방식
  - **Diffusion Policy (matched)**: OpenVLA의 입력 및 출력 사양과 일치하는 Diffusion Policy 버전
  - **Octo**: 대상 데이터셋에서 파인 튜닝된 모델. (RT-2-X는 추론 API를 통해 파인 튜닝 미지원) 현재 파인 튜닝을 지원하는 최고의 일반 정책
  - **OpenVLA**: 동일한 대상 데이터셋에서 파인 튜닝된 OpenVLA
  - **OpenVLA (scratch)**: OpenX 사전 훈련된 OpenVLA 모델 대신, 기본 Prismatic VLM을 대상 로봇 설정에서 직접 파인 튜닝한 버전 (대규모 로봇 사전 훈련의 이점 평가 목적)
- **결과**:
  - **Diffusion Policy (두 버전 모두)**: "당근을 그릇에 넣기", "옥수수를 냄비에 붓기"와 같은 좁은 단일 명령 작업에서 일반 정책 Octo 및 OpenVLA와 경쟁력이 있거나 능가함.
  - **사전 훈련된 일반 정책 (Octo, OpenVLA)**: 장면에서 여러 객체를 포함하고 언어 조건화가 필요한 더 다양한 파인 튜닝 작업에서 더 나은 성능을 보임.
    - OpenX 사전 훈련을 통해 Octo와 OpenVLA는 language grounding 중요한 이러한 다양한 작업에 더 잘 적응할 수 있음. (OpenVLA (scratch)의 낮은 성능이 이를 뒷받침)
  - **OpenVLA**: 전반적으로 가장 높은 평균 성능 달성
    - 대부분의 이전 작업은 좁은 단일 명령 또는 다양한 다중 명령 작업에서만 강력한 성능을 보여 성공률 편차가 큼.
    - OpenVLA는 테스트된 모든 작업에서 최소 50%의 성공률을 달성하는 유일한 접근 방식으로, 특히 다양한 언어 명령 세트가 포함된 모방 학습 작업에 대한 강력한 기본 옵션이 될 수 있음을 시사함.
  - **좁지만 고도로 숙련된 작업**: Diffusion Policy는 여전히 더 부드럽고 정확한 궤적을 보임.
  - **향후 작업 제안**: Diffusion Policy에 구현된 액션 청킹 및 시간적 평활화를 OpenVLA에 통합하면 동일한 수준의 숙련도 달성에 도움이 될 수 있음.

<br>

### 4.3 Parameter-Efficient Fine-Tuning
이전 섹션의 OpenVLA 전체 파인 튜닝은 높은 성능을 달성하기 위해 작업당 5~15시간 동안 8개의 A100 GPU를 사용했다. 이는 VLA 사전 훈련보다 훨씬 적은 계산량이지만, 이 섹션에서는 훨씬 더 컴퓨팅 및 파라미터 효율적인 파인 튜닝 접근 방식을 살펴보고 그 효과를 조사한다.

<figure align="center">
<center><img src="/assets/img/openvla/5.png" style="zoom: 80%;" /></center>
<figcaption>Table 1: Parameter-efficient fine-tuning evaluation.</figcaption>
</figure>

<br>

<figure align="center">
<center><img src="/assets/img/openvla/6.png" style="zoom: 110%;" /></center>
<figcaption>Table 8: Detailed parameter-efficient fine-tuning experiment results.</figcaption>
</figure>

<br>

- **비교 대상**:
  - **Full Fine-tuning**: 4.2절에서 설명한 대로 모든 가중치 업데이트
  - **Last Layer Only**: OpenVLA의 트랜스포머 백본과 토큰 임베딩 행렬의 마지막 레이어만 파인 튜닝
  - **Frozen Vision**: 비전 인코더를 고정하고 다른 모든 가중치를 파인 튜닝
  - **Sandwich Fine-tuning**: 비전 인코더, 토큰 임베딩 행렬, 마지막 레이어를 고정 해제하여 파인 튜닝
  - **LoRA (Low-Rank Adaptation)**: 모델의 모든 선형 레이어에 적용된 여러 rank 값 r을 가진 널리 사용되는 low-rank adaptation 기술
- **결과** (표 1; Franka-Tabletop 작업, 접근 방식당 33개 롤아웃; 자세한 내용은 표 8 참조):
  - **Last Layer Only / Frozen Vision**: 성능 저하 발생. 이는 대상 장면에 대한 시각적 특징의 추가 조정이 중요함을 시사함.
  - **Sandwich Fine-tuning**: 비전 인코더를 파인 튜닝하여 더 나은 성능을 달성하고, 전체 LLM 백본을 파인 튜닝하지 않아 GPU 메모리를 덜 소비함.
  - **LoRA**: 성능과 훈련 메모리 소비 간의 최상의 절충점을 달성함.
    - Sandwich Fine-tuning보다 성능이 우수
    - **파라미터의 1.4%만 파인 튜닝**하면서 Full Fine-tuning 성능과 일치함.
    - LoRA rank는 정책 성능에 미미한 영향을 미치므로 기본 rank `r=32` 사용 권장
    - LoRA 사용 시, 단일 A100 GPU에서 10~15시간 이내에 새로운 작업에서 OpenVLA 파인 튜닝 가능 (전체 Fine-tuning 대비 계산량 8배 감소)

<br>

### 4.4 Memory-Efficient Inference via Quantization
7B 파라미터 모델인 OpenVLA는 Octo와 같은 이전의 오픈 소스 제너럴리스트 정책(<100M 파라미터)보다 추론 시 더 많은 메모리를 소비한다. LLM 제공의 모범 사례를 따라 OpenVLA를 bfloat16 정밀도로 저장하고 로드하여, 메모리 사용량을 절반으로 줄여 16GB GPU에서도 OpenVLA를 제공할 수 있게 한다. 이 섹션에서는 LLM 제공을 위해 개발된 최신 양자화 기술을 사용하여 추론에 필요한 메모리를 더욱 줄이고 VLA의 접근성을 넓힐 수 있는지 테스트한다. 이러한 접근 방식은 네트워크 가중치를 더 낮은 정밀도로 로드하여 메모리 요구 사항을 줄이지만, 잠재적으로 추론 속도와 정확도를 낮출 수 있다.

<figure align="center">
<center><img src="/assets/img/openvla/7.png" style="zoom: 80%;" /></center>
<figcaption>Table 2: Performance with quantized inference.</figcaption>
</figure>

<br>

<figure align="center">
<center><img src="/assets/img/openvla/8.png" style="zoom: 80%;" /></center>
<figcaption>Figure 6: OpenVLA inference speed for various GPUs.</figcaption>
</figure>

<br>

- **실험 설정**: 8개의 대표적인 BridgeData V2 작업에서 8비트 및 4비트 정밀도로 OpenVLA 모델 제공 조사
- **결과** (표 2: 메모리 공간 및 롤아웃 성능; 그림 6: 다양한 GPU에서의 제어 빈도):
  - **8-bit quantization**:
    - 추가된 양자화 연산의 오버헤드로 인해 대부분의 GPU에서 추론 속도 저하 관찰
    - 상당한 성능 저하 관찰 (평가용 A5000 GPU에서 모델 1.2Hz 실행, 이는 BridgeData V2 작업의 5Hz 비차단 컨트롤러 훈련 데이터셋과 비교하여 시스템 역학을 크게 변경)
  - **4-bit quantization**:
    - GPU 메모리 전송 감소가 양자화 오버헤드를 보상하여 더 높은 처리량 달성
    - **GPU 메모리 양의 절반 미만을 필요**로 함에도 불구하고 **bfloat16 반정밀도 추론과 유사한 성능** 제공
    - 4비트 양자화 모델은 A5000에서 3Hz로 실행 가능하여 데이터 수집 중 시스템 역학과 더 밀접하게 일치함.

<br>

# 5. Discussion and Limitations
본 연구는 즉시 사용 가능한(out-of-the-box) 교차 구현 로봇 제어에서 강력한 성능을 보이는 SOTA 오픈소스 vision-language-action 모델인 **OpenVLA**를 제시했다. 또한 OpenVLA가 파라미터 효율적인 파인 튜닝 기술을 통해 새로운 로봇 설정에 쉽게 적용될 수 있음을 입증했다.

현재 OpenVLA 모델에는 다음과 같은 몇 가지 제한 사항이 있다.
- **단일 이미지 관찰만 지원**:
  - **문제점**: 실제 로봇 설정은 다양한 감각 입력(여러 이미지, 고유 감각, 관찰 기록 등)을 가짐.
  - **향후 연구 방향**: OpenVLA를 확장하여 이러한 다양한 입력을 지원하도록 하는 것이 중요함. interleaved 이미지 및 텍스트 데이터에서 사전 훈련된 VLM 사용 탐색이 유연한 입력 VLA 미세 조정을 용이하게 할 수 있음.
- **추론 처리량 개선 필요**:
  - **문제점**: 현재 추론 속도는 ALOHA (50Hz)와 같은 고주파 제어 설정을 위한 VLA 제어를 가능하게 하기엔 부족함.
  - **잠재적 해결책**: 더 능숙한 양손 조작 테스트를 위해 action chunking 또는 speculative decoding과 같은 대체 추론 시간 최적화 기술 탐색
- **성능 향상 여지**:
  - **현황**: OpenVLA는 이전 일반 정책보다 성능이 우수하지만, 일반적으로 90% 미만의 성공률을 보여 테스트된 작업에서 아직 매우 높은 신뢰성을 제공하지는 못함.
- **탐구되지 않은 VLA 설계 질문**:
  - **원인**: 계산 제약으로 인해 많은 VLA 설계에 대한 질문이 아직 충분히 탐구되지 못함.
  - **미해결 질문**:
    - base VLM의 크기가 VLA 성능에 미치는 영향은 무엇인가?
    - 로봇 행동 예측 데이터와 인터넷 규모의 vision-languate 데이터에 대한 co-training이 VLA 성능을 크게 향상시키는가?
    - VLA 모델에 가장 적합한 시각적 특징은 무엇인가?

<br>

# 6. Conclusions
Robot Foundation Model, 특히 **Vision-Language-Action Model(VLA)** 분야는 Google Deepmind와 NVIDIA가 양대 산맥으로 연구를 주도하고 있다. NVIDIA의 Isaac Groot 프로젝트나 Google의 OpenVLA 논문에서 볼 수 있듯, 각자의 VLA 모델 개발에 힘쓰면서도, 데이터와 기반 인프라가 부족한 로봇 분야의 특성상 **Newton 물리 엔진** 공동 개발이나 **MuJoCo 시뮬레이터** 최적화 같은 기초 기술 협력도 병행하는 양상을 보이고 있다. 이러한 흐름 속에서, 특히 Google의 OpenVLA 논문은 기존 VLA의 폐쇄적인 한계를 넘어 다양한 로봇과 작업에 일반화될 수 있는 개방형 표준 플랫폼을 구축하려는 중요한 시도를 보여주는 대표적인 사례이다. 이 논문이 제시하는 핵심은, 크고 다양한 로봇 데이터셋으로 강력한 VLM을 (특히 비전 부분을 포함하여) 충분한 에포크로 학습시키면, 일반화 성능이 뛰어나고 새로운 작업에도 쉽게 적응하며, LoRA나 양자화 같은 기술로 계산 효율성까지 확보할 수 있는 로봇 정책을 만들 수 있다는 점이다. **그렇다면 이처럼 미래 로봇 기술의 핵심으로 떠오르는 VLA란 정확히 무엇일까?** 다음 게시글에서는 VLA의 전반적인 개요와 함께 그 가능성을 함께 살펴보도록 하겠다.

<br>
